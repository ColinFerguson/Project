{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pdfminer import pdfparser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import string\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from nltk import PorterStemmer\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from cStringIO import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = file(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\\\n",
    "                                  password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import  TextConverter # , XMLConverter, HTMLConverter\n",
    "import urllib2\n",
    "from urllib2 import Request\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# Define a PDF parser function\n",
    "def parsePDF(url):\n",
    "\n",
    "    # Open the url provided as an argument to the function and read the content\n",
    "    open = urllib2.urlopen(Request(url)).read()\n",
    "\n",
    "    # Cast to StringIO object\n",
    "    from StringIO import StringIO\n",
    "    memory_file = StringIO(open)\n",
    "\n",
    "    # Create a PDF parser object associated with the StringIO object\n",
    "    parser = PDFParser(memory_file)\n",
    "\n",
    "    # Create a PDF document object that stores the document structure\n",
    "    document = PDFDocument(parser)\n",
    "\n",
    "    # Define parameters to the PDF device objet \n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    codec = 'utf-8'\n",
    "\n",
    "    # Create a PDF device object\n",
    "    device = TextConverter(rsrcmgr, retstr, codec = codec, laparams = laparams)\n",
    "\n",
    "    # Create a PDF interpreter object\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    # Process each page contained in the document\n",
    "    for page in PDFPage.create_pages(document):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    '''Function to pull all papers (pdf) off the arxiv'''\n",
    "    base_url = url\n",
    "    r = requests.get(base_url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    pdfs = soup.findAll(title = 'Download PDF')\n",
    "    links = [str(pdf).split()[1].strip('href=\"') for pdf in pdfs]\n",
    "    urls = ['http://arxiv.org'+ link for link in links]\n",
    "    articles = []\n",
    "    for url in urls:\n",
    "        articles.append(parsePDF(url))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#steph = convert_pdf_to_txt('/Users/Colin/Downloads/plms.pdf')\n",
    "#physics1 = convert_pdf_to_txt('/Users/Colin/Downloads/physics1.pdf')\n",
    "#shake = convert_pdf_to_txt('/Users/Colin/Downloads/shakespeare-tempest.pdf')\n",
    "#steph2 = convert_pdf_to_txt('/Users/Colin/Downloads/twists_final.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 2s, sys: 1.89 s, total: 8min 4s\n",
      "Wall time: 11min 13s\n"
     ]
    }
   ],
   "source": [
    "text= []\n",
    "%time text = get_text('http://arxiv.org/list/math.NT/1508?show=150')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try to get rid of weird unicode characters\n",
    "S = set()\n",
    "S.update(letter for letter in string.lowercase)\n",
    "S.update(letter for letter in string.uppercase)\n",
    "S.update(digit for digit in string.digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#text.append(steph)\n",
    "#text.append(shake)\n",
    "\n",
    "#f = lambda x: 1 if x in string.printable else 0\n",
    "#goodwords = []\n",
    "#words = ['abcd\\xce\\xc2fg', 'running', 'primes', 'groups']\n",
    "#for i in range(len(text)):\n",
    " #   goodwords.append([word for word in text[i].split() if all([f(char) for char in word])])\n",
    "\n",
    "\n",
    "new_text=[]\n",
    "for i in range(len(text)):\n",
    "    new_text.append([word for word in text[i].split() if (word[0] in S) and (word[-1] in S)\\\n",
    "                    and (len(word)>3)])\n",
    "   \n",
    "for i in range(len(new_text)):\n",
    "    new_text[i] = ' '.join(new_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make stop list\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "Stop= set()\n",
    "Stop.update([word for word in tfidf.get_stop_words()])\n",
    "Stop.update(['theorem', 'lemma', 'proof', 'sum', 'difference', \\\n",
    "             'product', 'multiple', 'let', 'group', 'prime', 'log', 'limit', 'cid', 'result'\\\n",
    "            'main', 'conjecture', 'case', 'suppose', 'function', 'assume', 'follows', \\\n",
    "            'given', 'define', 'note', 'defined', 'class', 'proposition', 'function', 'set', \\\n",
    "             'primes', 'numbers','form', 'integers', 'curves', 'real'])\n",
    "Stop = list(Stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#new_text.append(new_steph)\n",
    "tfidf_math = TfidfVectorizer(max_features=100, stop_words=Stop, ngram_range=(2,2))\n",
    "M = tfidf_math.fit_transform(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "lower bound upper bound suﬃciently large probability measure section prove number theory 2n 2n non zero algebraic number absolute value\n",
      "\n",
      "\n",
      "Topic #1:\n",
      "elliptic curve abelian variety modular forms abelian varieties algebraic closure lecture notes polynomial degree absolute value positive integer quadratic space\n",
      "\n",
      "\n",
      "Topic #2:\n",
      "12 12 mean value error term arithmetic properties right hand functional equation eisenstein series 2n 2n algebraic number cambridge university\n",
      "\n",
      "\n",
      "Topic #3:\n",
      "lattice points non negative probability measure non zero square free loss generality number theory section prove linear combination fourier coeﬃcients\n",
      "\n",
      "\n",
      "Topic #4:\n",
      "quadratic forms binary quadratic weakly holomorphic holomorphic modular maass forms quadratic space section prove loss generality positive integer positive proportion\n",
      "\n",
      "\n",
      "Topic #5:\n",
      "eisenstein series modular forms siegel modular fourier expansion maass forms exact sequence holomorphic modular number theory fourier coeﬃcients square free\n",
      "\n",
      "\n",
      "Topic #6:\n",
      "main result mathematics subject cambridge university gives extension probability measure mean value right hand upper bound positive integer mordell weil\n",
      "\n",
      "\n",
      "Topic #7:\n",
      "positive integer right hand suﬃciently large xj xj left hand non trivial mathematics subject error term non negative number theory\n",
      "\n",
      "\n",
      "Topic #8:\n",
      "xn xn functional equation error term number theory non trivial upper bound fourier expansion right hand algebraic number fourier coeﬃcients\n",
      "\n",
      "\n",
      "Topic #9:\n",
      "vector space linearly independent power series mathematics subject non archimedean polynomial degree abelian variety linear combination galois extension non zero\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get some topics\n",
    "\n",
    "feature_names = tfidf_math.get_feature_names()\n",
    "nmf = NMF(n_components=10)\n",
    "nmf.fit(M)\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111 101  99  65  71 102  25  37  19  56]\n",
      "[-0.          0.48054989  0.65083542  0.74890751  0.76693129  0.77536315\n",
      "  0.7944557   0.79469272  0.83804994  0.8450582 ]\n"
     ]
    }
   ],
   "source": [
    "#Get similarities\n",
    "N = M.todense()\n",
    "distances = []\n",
    "for ix in range(N.shape[0]):\n",
    "    distances.append(cosine(N[-3], N[ix]))\n",
    "print np.argsort(distances)[:10]\n",
    "print np.around(sorted(distances)[:10],9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dists = []\n",
    "for ix in range(N.shape[0]):\n",
    "    for jx in range(N.shape[0]):\n",
    "        dists.append(cosine(N[ix], N[jx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Snow = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = lambda x: 1 if x in string.printable else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colin\n",
      "is\n",
      "rad\n"
     ]
    }
   ],
   "source": [
    "flip = 'Colin is rad'\n",
    "for word in flip.split():\n",
    "    print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#base_url = 'http://arxiv.org/list/math.NT/pastweek?show=28'\n",
    "#from urllib import urlopen\n",
    "#webpage = urlopen(base_url)\n",
    "#soup=BeautifulSoup(webpage, \"html5lib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
