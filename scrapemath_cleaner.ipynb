{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pdfminer import pdfparser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import string\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from nltk import PorterStemmer\n",
    "from nltk import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from cStringIO import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = file(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\\\n",
    "                                  password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import  TextConverter # , XMLConverter, HTMLConverter\n",
    "import urllib2\n",
    "from urllib2 import Request\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# Define a PDF parser function\n",
    "def parsePDF(url):\n",
    "\n",
    "    # Open the url provided as an argument to the function and read the content\n",
    "    open = urllib2.urlopen(Request(url)).read()\n",
    "\n",
    "    # Cast to StringIO object\n",
    "    from StringIO import StringIO\n",
    "    memory_file = StringIO(open)\n",
    "\n",
    "    # Create a PDF parser object associated with the StringIO object\n",
    "    parser = PDFParser(memory_file)\n",
    "\n",
    "    # Create a PDF document object that stores the document structure\n",
    "    document = PDFDocument(parser)\n",
    "\n",
    "    # Define parameters to the PDF device objet \n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    codec = 'utf-8'\n",
    "\n",
    "    # Create a PDF device object\n",
    "    device = TextConverter(rsrcmgr, retstr, codec = codec, laparams = laparams)\n",
    "\n",
    "    # Create a PDF interpreter object\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    # Process each page contained in the document\n",
    "    for page in PDFPage.create_pages(document):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    '''Function to pull all papers (pdf) off the arxiv'''\n",
    "    base_url = url\n",
    "    r = requests.get(base_url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    pdfs = soup.findAll(title = 'Download PDF')\n",
    "    links = [str(pdf).split()[1].strip('href=\"') for pdf in pdfs]\n",
    "    urls = ['http://arxiv.org'+ link for link in links]\n",
    "    articles = []\n",
    "    for url in urls:\n",
    "        articles.append(parsePDF(url))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#steph = convert_pdf_to_txt('/Users/Colin/Downloads/plms.pdf')\n",
    "#physics1 = convert_pdf_to_txt('/Users/Colin/Downloads/physics1.pdf')\n",
    "#shake = convert_pdf_to_txt('/Users/Colin/Downloads/shakespeare-tempest.pdf')\n",
    "#steph2 = convert_pdf_to_txt('/Users/Colin/Downloads/twists_final.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 29s, sys: 3.32 s, total: 10min 33s\n",
      "Wall time: 13min 58s\n"
     ]
    }
   ],
   "source": [
    "text= []\n",
    "%time text = get_text('http://arxiv.org/list/math.NT/1508?show=150')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try to get rid of weird unicode characters\n",
    "S = set()\n",
    "S.update(letter for letter in string.lowercase)\n",
    "S.update(letter for letter in string.uppercase)\n",
    "S.update(digit for digit in string.digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#text.append(steph)\n",
    "#text.append(shake)\n",
    "\n",
    "#f = lambda x: 1 if x in string.printable else 0\n",
    "#goodwords = []\n",
    "#words = ['abcd\\xce\\xc2fg', 'running', 'primes', 'groups']\n",
    "#for i in range(len(text)):\n",
    " #   goodwords.append([word for word in text[i].split() if all([f(char) for char in word])])\n",
    "\n",
    "\n",
    "new_text=[]\n",
    "for i in range(len(text)):\n",
    "    new_text.append([word for word in text[i].split() if (word[0] in S) and (word[-1] in S)\\\n",
    "                    and (len(word)>3)])\n",
    "   \n",
    "for i in range(len(new_text)):\n",
    "    new_text[i] = ' '.join(new_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make stop list\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "Stop = set()\n",
    "Stop.update([word for word in tfidf.get_stop_words()])\n",
    "Stop.update(['theorem', 'lemma', 'proof', 'sum', 'difference', \\\n",
    "             'product', 'multiple', 'let', 'group', 'prime', 'log', 'limit', 'cid', 'result'\\\n",
    "            'main', 'conjecture', 'case', 'suppose', 'function', 'assume', 'follows', \\\n",
    "            'given', 'define', 'note', 'defined', 'class', 'proposition', 'function', 'set', \\\n",
    "             'primes', 'numbers','form', 'integers', 'curves', 'real', 'denote', 'extensions', 'result'])\n",
    "Stop = list(Stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#new_text.append(new_steph)\n",
    "tfidf_math = TfidfVectorizer(max_features=100, stop_words=Stop, ngram_range=(1,1), decode_error='ignore')\n",
    "M = tfidf_math.fit_transform(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_topics(url, num_topics):\n",
    "    '''Input: URL containing links to each document (pdf) in the corpus (i.e. arxiv)\n",
    "    Output: the num_topics most important topics from the corpus\n",
    "    '''\n",
    "    text = get_text(url)\n",
    "    clean_text = clean_pdf_text(text)\n",
    "\n",
    "\n",
    "    tfidf_math = TfidfVectorizer(max_features=100, stop_words=math_stop(), ngram_range=(1,1), decode_error='ignore')\n",
    "    M = tfidf_math.fit_transform(clean_text)\n",
    "\n",
    "    feature_names = tfidf_math.get_feature_names()\n",
    "    nmf = NMF(n_components=10)\n",
    "    nmf.fit(M)\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        topics.append((\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])))\n",
    "    return topics\n",
    "\n",
    "def clean_pdf_text(text):\n",
    "    '''Input: list of text documents, each element of the list is a single (long) string\n",
    "    Output: slightly cleaned up version, try to get rid of strange characters and short 'words'\n",
    "    '''\n",
    "    S = set()\n",
    "    S.update(letter for letter in string.lowercase)\n",
    "    S.update(letter for letter in string.uppercase)\n",
    "    S.update(digit for digit in string.digits)\n",
    "\n",
    "\n",
    "    new_text=[]\n",
    "    for i in range(len(text)):\n",
    "        new_text.append([word.lower() for word in text[i].split() if (word[0] in S) and (word[-1] in S)\\\n",
    "                    and (len(word)>3)])\n",
    "\n",
    "    for i in range(len(new_text)):\n",
    "        new_text[i] = ' '.join(new_text[i])\n",
    "\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def math_stop():\n",
    "    '''Add math specific words to the standard stop list'''\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    Stop = set()\n",
    "    Stop.update([word for word in tfidf.get_stop_words()])\n",
    "    Stop.update(['theorem', 'lemma', 'proof', 'sum', 'difference', \\\n",
    "                 'product', 'multiple', 'let', 'group', 'prime', 'log', 'limit', 'cid', 'result'\\\n",
    "                'main', 'conjecture', 'case', 'suppose', 'function', 'assume', 'follows', \\\n",
    "                'given', 'define', 'note', 'defined', 'class', 'proposition', 'function', 'set', \\\n",
    "                 'primes', 'numbers','form', 'integers', 'curves', 'real', 'result', 'denote', 'extensions'])\n",
    "    return list(Stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number integer positive natural denote using result second consider following\n",
      "\n",
      "\n",
      "galois abelian subgroup extension extensions groups image isomorphism element local\n",
      "\n",
      "\n",
      "zeta functions representation series integral formula type local remark deﬁned\n",
      "\n",
      "\n",
      "quadratic forms ideal ring small rank polynomials number linear lattice\n",
      "\n",
      "\n",
      "sequence deﬁned polynomials following positive denote measure holds prove representation\n",
      "\n",
      "\n",
      "modulo coeﬃcients formula polynomial integer series number lattice order section\n",
      "\n",
      "\n",
      "lattice measure dimension exists sets point points following space prove\n",
      "\n",
      "\n",
      "bound 18 inequality integral obtain constant large following series small\n",
      "\n",
      "\n",
      "modular adic forms series associated deﬁne space functions coeﬃcients deﬁned\n",
      "\n",
      "\n",
      "curve degree rational point polynomial irreducible points polynomials linear rank\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#topics = get_topics('http://arxiv.org/list/math.NT/1508?show=150', 10)\n",
    "for i in range(len(topics)):\n",
    "    print topics[i]\n",
    "\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "number bound xn 18 inequality large obtain positive constant integer\n",
      "\n",
      "\n",
      "Topic #1:\n",
      "galois abelian subgroup extension groups isomorphism image element local elements\n",
      "\n",
      "\n",
      "Topic #2:\n",
      "zeta functions series xn integral formula type deﬁned local values\n",
      "\n",
      "\n",
      "Topic #3:\n",
      "quadratic forms ideal number ring small rank lattice consider polynomials\n",
      "\n",
      "\n",
      "Topic #4:\n",
      "sequence number deﬁned integer following positive polynomials prove formula holds\n",
      "\n",
      "\n",
      "Topic #5:\n",
      "modulo coeﬃcients formula integer number series lattice polynomial positive xn\n",
      "\n",
      "\n",
      "Topic #6:\n",
      "measure dimension lattice sets exists point points following space dimensional\n",
      "\n",
      "\n",
      "Topic #7:\n",
      "representation positive non points exists number point second irreducible image\n",
      "\n",
      "\n",
      "Topic #8:\n",
      "modular adic forms series deﬁne functions associated space coeﬃcients deﬁned\n",
      "\n",
      "\n",
      "Topic #9:\n",
      "curve degree polynomial rational polynomials point points irreducible rank linear\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get some topics\n",
    "\n",
    "feature_names = tfidf_math.get_feature_names()\n",
    "nmf = NMF(n_components=10)\n",
    "nmf.fit(M)\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "print len(text)\n",
    "print len(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139  16  17  24  42]\n",
      "[-0.     0.298  0.335  0.338  0.373]\n"
     ]
    }
   ],
   "source": [
    "#Get similarities\n",
    "N = M.todense()\n",
    "distances = []\n",
    "for ix in range(N.shape[0]):\n",
    "    distances.append(cosine(N[-3], N[ix]))\n",
    "print np.argsort(distances)[:5]\n",
    "print np.around(sorted(distances)[:5],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Dists = np.zeros((N.shape[0], N.shape[0]))\n",
    "for ix in range(len(Dists)):\n",
    "    for jx in range(len(Dists)):\n",
    "        Dists[ix, jx] = cosine(N[ix], N[jx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_distances(M, index1, index2):\n",
    "    N = M.todense()\n",
    "    Dists = np.zeros((N.shape[0], N.shape[0]))\n",
    "    for ix in range(len(Dists)):\n",
    "        for jx in range(len(Dists)):\n",
    "            Dists[ix, jx] = cosine(N[ix], N[jx])\n",
    "    return Dists[index1, index2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.2204460492503131e-16"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_distances(M, 25, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(decode_error='ignore', stop_words=Stop, max_features=5000)\n",
    "CV = countvec.fit_transform(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "vocab=tuple(countvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA instance at 0x110a836c8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = lda.LDA(n_topics=10, n_iter=1500)\n",
    "lda_model.fit(CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_word = lda_model.topic_word_  # model.components_ also works\n",
    "n_top_words = 8\n",
    "topic_words = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words.append(np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c4 alternating eigenvarieties rouen proven revisited kedk\n",
      "\n",
      "\n",
      "ryan g0 enumerating em orthonormal shown hg\n",
      "\n",
      "\n",
      "main publishing c4 trial 3α laurent peter\n",
      "\n",
      "\n",
      "bp1q pδs positive lack decomposes characterize c4\n",
      "\n",
      "\n",
      "ag point finally katherine dred maxϕ omitted\n",
      "\n",
      "\n",
      "information signiﬁcantly northcott c4 fourth space quote\n",
      "\n",
      "\n",
      "elnj q1 4k matsumoto ge derangements idea\n",
      "\n",
      "\n",
      "improves daeyeoul infer hr pencil pk1 isometric\n",
      "\n",
      "\n",
      "cycles extensions descending average frames lines exhibiting\n",
      "\n",
      "\n",
      "p2z olya average u1 mi branched hσ2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(topic_words)):\n",
    "    print ' '.join(topic_words[i])\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic = lda_model.doc_topic_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 0 topic: [u'c4' u'alternating' u'eigenvarieties' u'rouen' u'proven' u'revisited'\n",
      " u'kedk']\n",
      "doc: 1 topic: [u'c4' u'alternating' u'eigenvarieties' u'rouen' u'proven' u'revisited'\n",
      " u'kedk']\n",
      "doc: 2 topic: [u'p2z' u'olya' u'average' u'u1' u'mi' u'branched' u'h\\u03c32']\n",
      "doc: 3 topic: [u'ag' u'point' u'finally' u'katherine' u'dred' u'max\\u03d5' u'omitted']\n",
      "doc: 4 topic: [u'improves' u'daeyeoul' u'infer' u'hr' u'pencil' u'pk1' u'isometric']\n",
      "doc: 5 topic: [u'improves' u'daeyeoul' u'infer' u'hr' u'pencil' u'pk1' u'isometric']\n",
      "doc: 6 topic: [u'ryan' u'g0' u'enumerating' u'em' u'orthonormal' u'shown' u'hg']\n",
      "doc: 7 topic: [u'c4' u'alternating' u'eigenvarieties' u'rouen' u'proven' u'revisited'\n",
      " u'kedk']\n",
      "doc: 8 topic: [u'main' u'publishing' u'c4' u'trial' u'3\\u03b1' u'laurent' u'peter']\n",
      "doc: 9 topic: [u'ag' u'point' u'finally' u'katherine' u'dred' u'max\\u03d5' u'omitted']\n"
     ]
    }
   ],
   "source": [
    "topic_most_pr = []\n",
    "for n in range(10):\n",
    "    topic_most_pr.append(doc_topic[n].argmax())\n",
    "\n",
    "for n in range(10):\n",
    "    print \"doc: {} topic: {}\".format(n, str(topic_words[topic_most_pr[n]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 9, 4, 7, 7, 1, 0, 2, 4]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_most_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'ag', u'point', u'finally', u'katherine', u'dred', u'max\\u03d5',\n",
       "       u'omitted'], \n",
       "      dtype='<U20')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words[topic_most_pr[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_url = 'http://arxiv.org/list/math.NT/recent'\n",
    "r = requests.get(base_url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "titles = soup.findAll(class_=\"list-title\")\n",
    "title_list = []\n",
    "for ix in range(len(titles)):\n",
    "    title_list.append(titles[ix].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_best_titles(M, index):\n",
    "    N=M.todense()\n",
    "    Dists=np.zeros((N.shape[0], N.shape[0]))\n",
    "    for ix in range(len(Dists)):\n",
    "        for jx in range(len(Dists)):\n",
    "            Dists[ix, jx]=cosine(N[ix], N[jx])\n",
    "    distances = np.around(Dists[index],3)\n",
    "    best_five = np.argsort(distances)[1:6]\n",
    "    best_titles = [title_list[i] for i in best_five]\n",
    "    return best_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\\nTitle: On the universality of the Epstein zeta function\\n',\n",
       " u'\\nTitle: Combinatorial analysis of polynomial flatness problem, Mahler measure  and ergodic theory\\n',\n",
       " u'\\nTitle: Asymptotic bases and probabilistic representation functions\\n',\n",
       " u'\\nTitle: Pell Numbers whose Euler Function is a Pell Number\\n',\n",
       " u'\\nTitle: Representation of positive integers by the form $x^3+y^3+z^3-3xyz$\\n']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_titles(M, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text2(url):\n",
    "    '''Input a URL from the arxiv (page of a list of papers), return a list of\n",
    "    parsed articles (list of strings)\n",
    "    '''\n",
    "\n",
    "    base_url = url\n",
    "    r = requests.get(base_url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    pdfs = soup.findAll(title = 'Download PDF')\n",
    "    links = [str(pdf).split()[1].strip('href=\"') for pdf in pdfs]\n",
    "    urls = ['http://arxiv.org'+ link for link in links]\n",
    "    articles = []\n",
    "    for url in urls:\n",
    "        articles.append(parsePDF(url))\n",
    "\n",
    "    titles = soup.findAll(class_=\"list-title\")\n",
    "    title_list = []\n",
    "    for ix in range(len(titles)):\n",
    "        title_list.append(titles[ix].text)\n",
    "\n",
    "    return articles, title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text, title_list = get_text2('http://arxiv.org/list/math.NT/recent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_text=[]\n",
    "for i in range(len(text)):\n",
    "    new_text.append([word for word in text[i].split() if (word[0] in S) and (word[-1] in S)\\\n",
    "                    and (len(word)>3)])\n",
    "   \n",
    "for i in range(len(new_text)):\n",
    "    new_text[i] = ' '.join(new_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_math = TfidfVectorizer(max_features=100, stop_words=Stop, ngram_range=(1,1), decode_error='ignore')\n",
    "M = tfidf_math.fit_transform(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Backward Orbit Conjecture for the Powering Map over Global Fields\n",
      "\n",
      "\n",
      "Title: Additive equations in dense variables, and truncated restriction  estimates\n",
      "\n",
      "\n",
      "Title: On some upper bounds for the zeta-function and the Dirichlet divisor  problem\n",
      "\n",
      "\n",
      "Title: The class number one problem for the real quadratic fields  $\\mathbb{Q}\\left(\\sqrt{(an)^2+4a}\\right)$\n",
      "\n",
      "\n",
      "Title: A simple proof of Bourgain's theorem on the singularity of the spectrum  of Ornstein's maps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for title in get_best_titles(M,4):\n",
    "    print title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
