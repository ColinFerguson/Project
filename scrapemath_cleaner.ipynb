{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pdfminer import pdfparser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import string\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from nltk import PorterStemmer\n",
    "from nltk import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from cStringIO import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = file(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\\\n",
    "                                  password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import  TextConverter # , XMLConverter, HTMLConverter\n",
    "import urllib2\n",
    "from urllib2 import Request\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# Define a PDF parser function\n",
    "def parsePDF(url):\n",
    "\n",
    "    # Open the url provided as an argument to the function and read the content\n",
    "    open = urllib2.urlopen(Request(url)).read()\n",
    "\n",
    "    # Cast to StringIO object\n",
    "    from StringIO import StringIO\n",
    "    memory_file = StringIO(open)\n",
    "\n",
    "    # Create a PDF parser object associated with the StringIO object\n",
    "    parser = PDFParser(memory_file)\n",
    "\n",
    "    # Create a PDF document object that stores the document structure\n",
    "    document = PDFDocument(parser)\n",
    "\n",
    "    # Define parameters to the PDF device objet \n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    codec = 'utf-8'\n",
    "\n",
    "    # Create a PDF device object\n",
    "    device = TextConverter(rsrcmgr, retstr, codec = codec, laparams = laparams)\n",
    "\n",
    "    # Create a PDF interpreter object\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    # Process each page contained in the document\n",
    "    for page in PDFPage.create_pages(document):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    '''Function to pull all papers (pdf) off the arxiv'''\n",
    "    base_url = url\n",
    "    r = requests.get(base_url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    pdfs = soup.findAll(title = 'Download PDF')\n",
    "    links = [str(pdf).split()[1].strip('href=\"') for pdf in pdfs]\n",
    "    urls = ['http://arxiv.org'+ link for link in links]\n",
    "    articles = []\n",
    "    for url in urls:\n",
    "        articles.append(parsePDF(url))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#steph = convert_pdf_to_txt('/Users/Colin/Downloads/plms.pdf')\n",
    "#physics1 = convert_pdf_to_txt('/Users/Colin/Downloads/physics1.pdf')\n",
    "#shake = convert_pdf_to_txt('/Users/Colin/Downloads/shakespeare-tempest.pdf')\n",
    "#steph2 = convert_pdf_to_txt('/Users/Colin/Downloads/twists_final.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 5s, sys: 2.21 s, total: 8min 7s\n",
      "Wall time: 40min 13s\n"
     ]
    }
   ],
   "source": [
    "text= []\n",
    "%time text = get_text('http://arxiv.org/list/math.NT/1508?show=150')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try to get rid of weird unicode characters\n",
    "S = set()\n",
    "S.update(letter for letter in string.lowercase)\n",
    "S.update(letter for letter in string.uppercase)\n",
    "S.update(digit for digit in string.digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#text.append(steph)\n",
    "#text.append(shake)\n",
    "\n",
    "#f = lambda x: 1 if x in string.printable else 0\n",
    "#goodwords = []\n",
    "#words = ['abcd\\xce\\xc2fg', 'running', 'primes', 'groups']\n",
    "#for i in range(len(text)):\n",
    " #   goodwords.append([word for word in text[i].split() if all([f(char) for char in word])])\n",
    "\n",
    "\n",
    "new_text=[]\n",
    "for i in range(len(text)):\n",
    "    new_text.append([word for word in text[i].split() if (word[0] in S) and (word[-1] in S)\\\n",
    "                    and (len(word)>3)])\n",
    "   \n",
    "for i in range(len(new_text)):\n",
    "    new_text[i] = ' '.join(new_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make stop list\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "Stop= set()\n",
    "Stop.update([word for word in tfidf.get_stop_words()])\n",
    "Stop.update(['theorem', 'lemma', 'proof', 'sum', 'difference', \\\n",
    "             'product', 'multiple', 'let', 'group', 'prime', 'log', 'limit', 'cid', 'result'\\\n",
    "            'main', 'conjecture', 'case', 'suppose', 'function', 'assume', 'follows', \\\n",
    "            'given', 'define', 'note', 'defined', 'class', 'proposition', 'function', 'set', \\\n",
    "             'primes', 'numbers','form', 'integers', 'curves', 'real'])\n",
    "Stop = list(Stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#new_text.append(new_steph)\n",
    "tfidf_math = TfidfVectorizer(max_features=100, stop_words=Stop, ngram_range=(2,2), decode_error='ignore')\n",
    "M = tfidf_math.fit_transform(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "lower bound upper bound suﬃciently large probability measure section prove number theory 2n 2n non zero algebraic number absolute value\n",
      "\n",
      "\n",
      "Topic #1:\n",
      "elliptic curve abelian variety modular forms abelian varieties algebraic closure lecture notes polynomial degree absolute value positive integer quadratic space\n",
      "\n",
      "\n",
      "Topic #2:\n",
      "12 12 mean value error term arithmetic properties right hand functional equation eisenstein series 2n 2n algebraic number cambridge university\n",
      "\n",
      "\n",
      "Topic #3:\n",
      "lattice points non negative probability measure non zero square free loss generality number theory section prove linear combination fourier coeﬃcients\n",
      "\n",
      "\n",
      "Topic #4:\n",
      "quadratic forms binary quadratic weakly holomorphic holomorphic modular maass forms quadratic space section prove loss generality positive integer positive proportion\n",
      "\n",
      "\n",
      "Topic #5:\n",
      "eisenstein series modular forms siegel modular fourier expansion maass forms exact sequence holomorphic modular number theory fourier coeﬃcients square free\n",
      "\n",
      "\n",
      "Topic #6:\n",
      "main result mathematics subject cambridge university gives extension probability measure mean value right hand upper bound positive integer mordell weil\n",
      "\n",
      "\n",
      "Topic #7:\n",
      "positive integer right hand suﬃciently large xj xj left hand non trivial mathematics subject error term non negative number theory\n",
      "\n",
      "\n",
      "Topic #8:\n",
      "xn xn functional equation error term number theory non trivial upper bound fourier expansion right hand algebraic number fourier coeﬃcients\n",
      "\n",
      "\n",
      "Topic #9:\n",
      "vector space linearly independent power series mathematics subject non archimedean polynomial degree abelian variety linear combination galois extension non zero\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get some topics\n",
    "\n",
    "feature_names = tfidf_math.get_feature_names()\n",
    "nmf = NMF(n_components=10)\n",
    "nmf.fit(M)\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111 101  99  65  71 102  25  37  19  56]\n",
      "[-0.          0.48054989  0.65083542  0.74890751  0.76693129  0.77536315\n",
      "  0.7944557   0.79469272  0.83804994  0.8450582 ]\n"
     ]
    }
   ],
   "source": [
    "#Get similarities\n",
    "N = M.todense()\n",
    "distances = []\n",
    "for ix in range(N.shape[0]):\n",
    "    distances.append(cosine(N[-3], N[ix]))\n",
    "print np.argsort(distances)[:10]\n",
    "print np.around(sorted(distances)[:10],9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(decode_error='ignore', stop_words=Stop, max_features=5000)\n",
    "CV = countvec.fit_transform(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "vocab=tuple(countvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA instance at 0x10ccb5878>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = lda.LDA(n_topics=10, n_iter=1500)\n",
    "lda_model.fit(CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_word = lda_model.topic_word_  # model.components_ also works\n",
    "n_top_words = 8\n",
    "topic_words = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words.append(np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rest jγ approach plugging équation inductively extends\n",
      "\n",
      "\n",
      "necessary hida simpler v2i projective simplex coordinate\n",
      "\n",
      "\n",
      "monotonicity fractions helpful fil valid specok monogenic\n",
      "\n",
      "\n",
      "probabilistic submatrix inspection 98 duality process weyl\n",
      "\n",
      "\n",
      "williams border treatment implying unperturbed decimal submodule\n",
      "\n",
      "\n",
      "au intersecting attacks p1p2 denominators generates tensoring\n",
      "\n",
      "\n",
      "fractions qinghai area 4n inspection image 4ν2\n",
      "\n",
      "\n",
      "jh 3b2 td thompson functional gives jk\n",
      "\n",
      "\n",
      "approach inductively conductors landau milnor 54 coverings\n",
      "\n",
      "\n",
      "diﬀeomorphism direct killing presented suﬃcient 65 10jδj\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(topic_words)):\n",
    "    print ' '.join(topic_words[i])\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic = lda_model.doc_topic_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 0 topic: [u'monotonicity' u'fractions' u'helpful' u'fil' u'valid' u'specok'\n",
      " u'monogenic']\n",
      "doc: 1 topic: [u'monotonicity' u'fractions' u'helpful' u'fil' u'valid' u'specok'\n",
      " u'monogenic']\n",
      "doc: 2 topic: [u'probabilistic' u'submatrix' u'inspection' u'98' u'duality' u'process'\n",
      " u'weyl']\n",
      "doc: 3 topic: [u'rest' u'j\\u03b3' u'approach' u'plugging' u'\\xe9quation' u'inductively'\n",
      " u'extends']\n",
      "doc: 4 topic: [u'rest' u'j\\u03b3' u'approach' u'plugging' u'\\xe9quation' u'inductively'\n",
      " u'extends']\n",
      "doc: 5 topic: [u'williams' u'border' u'treatment' u'implying' u'unperturbed' u'decimal'\n",
      " u'submodule']\n",
      "doc: 6 topic: [u'approach' u'inductively' u'conductors' u'landau' u'milnor' u'54'\n",
      " u'coverings']\n",
      "doc: 7 topic: [u'monotonicity' u'fractions' u'helpful' u'fil' u'valid' u'specok'\n",
      " u'monogenic']\n",
      "doc: 8 topic: [u'jh' u'3b2' u'td' u'thompson' u'functional' u'gives' u'jk']\n",
      "doc: 9 topic: [u'monotonicity' u'fractions' u'helpful' u'fil' u'valid' u'specok'\n",
      " u'monogenic']\n"
     ]
    }
   ],
   "source": [
    "topic_most_pr = []\n",
    "for n in range(10):\n",
    "    topic_most_pr.append(doc_topic[n].argmax())\n",
    "\n",
    "for n in range(10):\n",
    "    print \"doc: {} topic: {}\".format(n, str(topic_words[topic_most_pr[n]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 3, 0, 0, 4, 8, 2, 7, 2]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_most_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'rest', u'j\\u03b3', u'approach', u'plugging', u'\\xe9quation',\n",
       "       u'inductively', u'extends'], \n",
       "      dtype='<U17')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words[topic_most_pr[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
