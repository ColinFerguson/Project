{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import cPickle as pickle\n",
    "import time\n",
    "from math_scraping_and_recommending_functions import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the Number Theory papers from the first nine months of 2015, easy to extend the code for more dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = []\n",
    "year = '15'\n",
    "for i in xrange(9,0,-1):\n",
    "    dates.append(year+'0'+str(i))\n",
    "front = 'http://lanl.arxiv.org/list/math.NT/'\n",
    "end = '?show=250'\n",
    "NT_urls = [front+x+end for x in dates]\n",
    "NT_names = ['NT_'+date+'.pkl' for date in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now get the NT data\n",
    "for url, name in zip(NT_urls, NT_names):\n",
    "    arts = get_text2(small_NT_urls[8])\n",
    "    good_type = 0\n",
    "    bad_type = 0\n",
    "    good_arts = []\n",
    "    for ix in range(len(arts)):\n",
    "        if type(arts[ix]) == tuple:  #Toss out the articles that don't render correctly on the arxiv\n",
    "            good_type += 1\n",
    "            good_arts.append(arts[ix])\n",
    "        else:\n",
    "            bad_type += 1\n",
    "    print good_type\n",
    "    print bad_type\n",
    "    \n",
    "    with open(name, 'w') as f:\n",
    "        pickle.dump(good_arts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want general mathematics data.  We'll scrape three months from each subject, June-August 2015.  This nets ~9000 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'math.NA', u'math.DG', u'math.HO', u'math.FA', u'math.DS', u'math.PR', u'math.LO', u'math.RA', u'math.NT', u'math.GR', u'math.CO', u'math.AG', u'math.GT', u'math.AC', u'math.IT', u'math.CA', u'math.AT', u'math.AP', u'math.CV', u'math.RT', u'math.CT', u'math.GN', u'math.GM', u'math.MG', u'math.OC', u'math.ST', u'math.OA', u'math.SP', u'math.MP', u'math.KT', u'math.QA', u'math.SG']\n"
     ]
    }
   ],
   "source": [
    "# First we'll get tags for each subject category\n",
    "r = requests.get('http://lanl.arxiv.org/archive/math', headers={'User-agent': 'Mozilla/5.0'})\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "match_math = re.findall(r'math\\.\\w\\w', soup.text)\n",
    "match_math = list(set(match_math))\n",
    "print match_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now build the URLs to be scraped and the names to save the data under\n",
    "front = 'http://lanl.arxiv.org/list/'\n",
    "end_june = '/1506?show=250'\n",
    "end_july = '/1507?show=250'\n",
    "end_aug = '/1508?show=250'\n",
    "\n",
    "math_urls_june = [front+x+end_june for x in match_math]\n",
    "math_urls_july = [front+x+end_july for x in match_math]\n",
    "math_urls_aug = [front+x+end_aug for x in match_math]\n",
    "\n",
    "math_tags = [x[-2:] for x in match_math]\n",
    "\n",
    "math_names_june = ['math_'+ x + '_June.pkl' for x in math_tags]\n",
    "math_names_july = ['math_'+ x + '_July.pkl' for x in math_tags]\n",
    "math_names_aug = ['math_'+ x + '_August.pkl' for x in math_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scrape it!!  We'll just go month by month, you need to be careful not to get kicked off the arxiv\n",
    "# Probably a good idea to break these lists up and do 5-6 categories at a time\n",
    "\n",
    "for url, name, tag in zip(math_urls_june, math_names_june, math_tags):\n",
    "    arts = get_text2(url)\n",
    "    bad_type = 0\n",
    "    good_type = 0\n",
    "    good_arts = []\n",
    "    for ix in range(len(arts)):\n",
    "        if type(arts[ix]) == tuple:\n",
    "            good_type += 1\n",
    "            good_arts.append(arts[ix])\n",
    "        else:\n",
    "            bad_type += 1\n",
    "    print \"Good: \", good_type           #Print statements aren't necessary, but I like to see what's going on\n",
    "    print \"Bad: \", bad_type\n",
    "    \n",
    "    for ix in range(len(good_arts)):\n",
    "        good_arts[ix] += (tag,)\n",
    "        with open(name, 'w') as f:\n",
    "            pickle.dump(good_arts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
